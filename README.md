# Nano-vLLM

A lightweight vLLM implementation built from scratch with comprehensive training capabilities.

## Key Features

### Inference
* ğŸš€ **Fast offline inference** - Comparable inference speeds to vLLM
* ğŸ“– **Readable codebase** - Clean implementation in ~ 1,200 lines of Python code
* âš¡ **Optimization Suite** - Prefix caching, Tensor Parallelism, Torch compilation, CUDA graph, etc.

### Training (NEW!)
* ğŸ”¥ **Full Training Support** - Train language models from scratch
* ğŸŒ **Distributed Training** - Tensor parallelism and data parallelism
* ğŸ’¾ **Memory Optimization** - Gradient checkpointing, mixed precision (FP16/BF16)
* ğŸ“Š **Advanced Optimizers** - AdamW with learning rate scheduling
* ğŸ¯ **Gradient Management** - Accumulation, clipping, and checkpointing
* ğŸ’¿ **Checkpoint Management** - Automatic saving, loading, and resuming
* ğŸ“ˆ **Evaluation** - Built-in metrics and evaluation during training

## Installation

```bash
pip install git+https://github.com/GeeeekExplorer/nano-vllm.git
```

## Quick Start

### Inference

See `example.py` for usage. The API mirrors vLLM's interface with minor differences in the `LLM.generate` method.
```python
from nanovllm import LLM, SamplingParams
llm = LLM("/YOUR/MODEL/PATH", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]
```

### Training

Train language models from scratch with nano-vllm:

```python
from nanovllm import Trainer, TrainingConfig

# Create training configuration
config = TrainingConfig(
    model_name_or_path="Qwen/Qwen3-0.6B",
    dataset_path="data/train.jsonl",
    output_dir="./checkpoints",
    learning_rate=5e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    mixed_precision="bf16",
    gradient_checkpointing=True,
)

# Create and run trainer
trainer = Trainer.from_pretrained(config, tokenizer)
trainer.train()
```

Or use the training script:

```bash
# Single GPU training
python train.py --config configs/train_config.json

# Multi-GPU distributed training
torchrun --nproc_per_node=4 train.py --config configs/train_config.json
```

## Benchmark

See `bench.py` for benchmark.

**Test Configuration:**
- Hardware: RTX 4070 Laptop (8GB)
- Model: Qwen3-0.6B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100â€“1024 tokens
- Output Length: Randomly sampled between 100â€“1024 tokens

**Performance Results:**
| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|----------------|-------------|----------|-----------------------|
| vLLM           | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM      | 133,966     | 93.41    | 1434.13               |


## Training Documentation

For comprehensive training documentation, see [TRAINING.md](TRAINING.md) which covers:

- ğŸ”§ **Configuration**: Detailed configuration options
- ğŸš€ **Distributed Training**: Tensor and data parallelism setup
- ğŸ’¾ **Memory Optimization**: Gradient checkpointing and mixed precision
- ğŸ“Š **Advanced Features**: Custom training loops, evaluation, checkpointing
- ğŸ› ï¸ **Troubleshooting**: Common issues and performance tips
- ğŸ“ **Examples**: Complete training examples and use cases

## Architecture

Nano-vLLM now supports both inference and training with a unified architecture:

```
nanovllm/
â”œâ”€â”€ engine/          # Inference engine (original)
â”œâ”€â”€ layers/          # Neural network layers (shared)
â”œâ”€â”€ models/          # Model implementations (enhanced for training)
â”œâ”€â”€ training/        # Training framework (NEW!)
â”‚   â”œâ”€â”€ trainer.py   # Main training engine
â”‚   â”œâ”€â”€ data.py      # Data loading utilities
â”‚   â”œâ”€â”€ optimizer.py # Optimizers and schedulers
â”‚   â”œâ”€â”€ evaluation.py # Evaluation utilities
â”‚   â”œâ”€â”€ checkpoint.py # Checkpoint management
â”‚   â””â”€â”€ memory.py    # Memory optimization
â”œâ”€â”€ config.py        # Configuration (enhanced)
â””â”€â”€ llm.py          # Main LLM interface
```

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=GeeeekExplorer/nano-vllm&type=Date)](https://www.star-history.com/#GeeeekExplorer/nano-vllm&Date)